{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adversarial Autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/danielcanueto/Adversarial_Autoencoder/blob/master/Adversarial_Autoencoder_2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "IOWEw65c-x-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWMmTKxy3Yb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#I've prepared a fork of the original repository in order to edit the parameters (e.g. from 1000 epochs to 100, as I've seen it is not necessary to have som amny at least during exploratory analysis).\n",
        "#!git clone https://github.com/danielcanueto/Adversarial_Autoencoder/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUApQmO43kfH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#os.chdir('Adversarial_Autoencoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nkEBRHdsMYjl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!python3 autoencoder.py --train True\n",
        "#!python3 adversarial_autoencoder.py --train True\n",
        "#!python3 supervised_adversarial_autoencoder.py --train True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_qp8xuDLiQJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!python3 adversarial_autoencoder.py --train False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koJ4jDefmkU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#To have access to each model saved. A folder for every model, with basic info in folder\n",
        "#os.listdir('Results/Adversarial_Autoencoder/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIAROAscnSSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Log info of training\n",
        "#data_path = 'Results/Adversarial_Autoencoder/2018-05-13 07:03:34.122351_2_0.001_100_100_0.9_Adversarial_Autoencoder/log/log.txt'\n",
        "#with open(data_path, 'rb') as f:\n",
        "#    lines = [x.decode('utf8').strip() for x in f.readlines()]\n",
        "#lines\n",
        "\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "#mnist = input_data.read_data_sets('./Data', one_hot=True)\n",
        "\n",
        "#print(mnist.train.images.shape, mnist.train.labels.shape)\n",
        "#print(mnist.validation.images.shape, mnist.validation.labels.shape)\n",
        "#print(mnist.test.images.shape, mnist.test.labels.shape)\n",
        "#mnist.train.next_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EOw9USP4erZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#os.chdir(\"..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzmrWiGC1bTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c7732253-cf26-4671-aecc-eb0dca2ced84"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#I've prepared a fork of the original repository to handle python 2 -> python 3 changes\n",
        "!git clone https://github.com/danielcanueto/abide"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'abide'...\n",
            "remote: Counting objects: 869, done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 869 (delta 13), reused 0 (delta 0), pack-reused 845\u001b[K\n",
            "Receiving objects: 100% (869/869), 67.23 MiB | 21.16 MiB/s, done.\n",
            "Resolving deltas: 100% (445/445), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_gGw82-S1o8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9aa1a675-0eff-4afe-947e-08f7696a3269"
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"abide\")\n",
        "os.listdir()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['freesurfer_files.json',\n",
              " 'README.md',\n",
              " 'map_rawout_to_organized_s3.py',\n",
              " 'concordances',\n",
              " 'download_abide_preproc.py',\n",
              " '.git',\n",
              " '.gitignore',\n",
              " 'download_abide_preproc_guide.txt',\n",
              " 'preprocessing']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "sFe6RV0Z2rYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python3 download_abide_preproc.py -d reho -p cpac -s nofilt_noglobal -o '/content/ABIDE_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nzre5_V59bs1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#os.listdir(\"/content/ABIDE_data/Outputs/cpac/nofilt_noglobal/reho/\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGLkW6-cpcZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1140
        },
        "outputId": "62abff31-a27f-4c05-8901-dcbdb40e41b9"
      },
      "cell_type": "code",
      "source": [
        "!pip install nilearn\n",
        "MNI152_FILE_PATH=\"/content/ABIDE_data/Outputs/cpac/nofilt_noglobal/reho/OHSU_0050148_reho.nii.gz\"\n",
        "from nilearn import plotting\n",
        "plotting.plot_img(MNI152_FILE_PATH)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/a2/0a528aff1c755f98538b18bd49765eae83c6a4da99b27f51baddb6ec5b45/nilearn-0.4.1.tar.gz (894kB)\n",
            "\u001b[K    100% |████████████████████████████████| 901kB 14.3MB/s \n",
            "\u001b[?25hCollecting nibabel>=2.0.2 (from nilearn)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/1d96fd0b118c9047bf35f02090db8ef8fd3927dfce635f09a6f7d5b572e6/nibabel-2.2.1.zip (4.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.2MB 10.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nilearn, nibabel\n",
            "  Running setup.py bdist_wheel for nilearn ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ee/09/3c/6ddb63159d6377d2c1587e6b4d58a6456da4550c2248583ffc\n",
            "  Running setup.py bdist_wheel for nibabel ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/46/50/8d/bcb0b8f7c030da5bac1752fbe9cc375cbf5725fa93ba79ad84\n",
            "Successfully built nilearn nibabel\n",
            "Installing collected packages: nibabel, nilearn\n",
            "Successfully installed nibabel-2.2.1 nilearn-0.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nilearn/plotting/__init__.py:20: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 657, in launch_instance\n",
            "    app.initialize(argv)\n",
            "  File \"<decorator-gen-121>\", line 2, in initialize\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 87, in catch_config_error\n",
            "    return method(app, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 462, in initialize\n",
            "    self.init_gui_pylab()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 403, in init_gui_pylab\n",
            "    InteractiveShellApp.init_gui_pylab(self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/shellapp.py\", line 213, in init_gui_pylab\n",
            "    r = enable(key)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nilearn.plotting.displays.OrthoSlicer at 0x7f2e2506a080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAADeCAYAAABYBPh4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEIIIVTIACGEEEIIFTJACCGEEEKF/wfSYnZw8cLE4QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2e2bb7c240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "juOwf2qQvmxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81785cac-1fe6-4d07-be0b-d926dca5a550"
      },
      "cell_type": "code",
      "source": [
        "!pip install nibabel\n",
        "import nibabel as nib\n",
        "img = nib.load(MNI152_FILE_PATH)\n",
        "a = np.array(img.dataobj)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (2.2.1)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q4BMUKMRwdfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dirlist=os.listdir(\"/content/ABIDE_data/Outputs/cpac/nofilt_noglobal/reho/\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qz1cSPIiJVKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e9379eb-94cd-4ba7-a150-cd022411c8c1"
      },
      "cell_type": "code",
      "source": [
        "a=np.zeros((len(dirlist),a2))\n",
        "a.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(884, 3721)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "McHZFceVyq8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for x in range(len(dirlist)):\n",
        "  img = nib.load(os.path.join(\"/content/ABIDE_data/Outputs/cpac/nofilt_noglobal/reho/\", dirlist[x]))\n",
        "  a1 = np.array(img.dataobj)[:,32,:]\n",
        "  a[x,] = a1.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W0dHm7tiNbAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d6f57c7a-55f4-4aca-c83c-e1eaa757e5a8"
      },
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "stats.describe(a1.ravel())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DescribeResult(nobs=3721, minmax=(0.0, 0.30123770236968994), mean=0.04843139977278716, variance=0.00395184063713764, skewness=1.1291287801224423, kurtosis=0.44424546538537113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "2xoWwR7rvuv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_tf = tf.convert_to_tensor(a, np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBeKNf5P27ao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1ee5a434-457f-41f2-9168-beb27f85925f"
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "!git clone https://github.com/danielcanueto/Adversarial_Autoencoder/\n",
        "os.chdir(\"/content/Adversarial_Autoencoder\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Adversarial_Autoencoder'...\n",
            "remote: Counting objects: 285, done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 285 (delta 7), reused 3 (delta 0), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (285/285), 9.04 MiB | 12.81 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HuikmWA84F5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!python3 dummy.py --train True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZ51HkDaG1Co",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Progressbar\n",
        "# bar = progressbar.ProgressBar(widgets=['[', progressbar.Timer(), ']', progressbar.Bar(), '(', progressbar.ETA(), ')'])\n",
        "\n",
        "# Get the MNIST data\n",
        "mnist = data_tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HyZZ_xg6ISYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd8cf6f8-2601-4cfe-81d9-03a57f975c97"
      },
      "cell_type": "code",
      "source": [
        "mnist.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(884), Dimension(3721)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "ROTl9Sa7HwUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#mnist = input_data.read_data_sets('./Data', one_hot=True)\n",
        "#mnist = data_tf\n",
        "#print(mnist.train.images.shape, mnist.train.labels.shape)\n",
        "#print(mnist.validation.images.shape, mnist.validation.labels.shape)\n",
        "#print(mnist.test.images.shape, mnist.test.labels.shape)\n",
        "#mnist.train.next_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "thziISL7Hh1o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_batch(self, batch_size, fake_data=False):\n",
        "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "    if fake_data:\n",
        "      fake_image = [1] * 784\n",
        "      if self.one_hot:\n",
        "        fake_label = [1] + [0] * 9\n",
        "      else:\n",
        "        fake_label = 0\n",
        "      return [fake_image for _ in xrange(batch_size)], [\n",
        "          fake_label for _ in xrange(batch_size)\n",
        "      ]\n",
        "    start = self._index_in_epoch\n",
        "    self._index_in_epoch += batch_size\n",
        "    if self._index_in_epoch > self._num_examples:\n",
        "      # Finished epoch\n",
        "      self._epochs_completed += 1\n",
        "      # Shuffle the data\n",
        "      perm = numpy.arange(self._num_examples)\n",
        "      numpy.random.shuffle(perm)\n",
        "      self._images = self._images[perm]\n",
        "      self._labels = self._labels[perm]\n",
        "      # Start next epoch\n",
        "      start = 0\n",
        "      self._index_in_epoch = batch_size\n",
        "      assert batch_size <= self._num_examples\n",
        "    end = self._index_in_epoch\n",
        "    return self._images[start:end], self._labels[start:end]\n",
        "\n",
        "\n",
        "def form_results():\n",
        "    \"\"\"\n",
        "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
        "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
        "    \"\"\"\n",
        "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Adversarial_Autoencoder\". \\\n",
        "        format(datetime.datetime.now(), z_dim, learning_rate, batch_size, n_epochs, beta1)\n",
        "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
        "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
        "    log_path = results_path + folder_name + '/log'\n",
        "    if not os.path.exists(results_path + folder_name):\n",
        "        os.mkdir(results_path + folder_name)\n",
        "        os.mkdir(tensorboard_path)\n",
        "        os.mkdir(saved_model_path)\n",
        "        os.mkdir(log_path)\n",
        "    return tensorboard_path, saved_model_path, log_path\n",
        "\n",
        "\n",
        "def generate_image_grid(sess, op):\n",
        "    \"\"\"\n",
        "    Generates a grid of images by passing a set of numbers to the decoder and getting its output.\n",
        "    :param sess: Tensorflow Session required to get the decoder output\n",
        "    :param op: Operation that needs to be called inorder to get the decoder output\n",
        "    :return: None, displays a matplotlib window with all the merged images.\n",
        "    \"\"\"\n",
        "    x_points = np.arange(-10, 10, 1.5).astype(np.float32)\n",
        "    y_points = np.arange(-10, 10, 1.5).astype(np.float32)\n",
        "\n",
        "    nx, ny = len(x_points), len(y_points)\n",
        "    plt.subplot()\n",
        "    gs = gridspec.GridSpec(nx, ny, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i, g in enumerate(gs):\n",
        "        z = np.concatenate(([x_points[int(i / ny)]], [y_points[int(i % nx)]]))\n",
        "        z = np.reshape(z, (1, 2))\n",
        "        x = sess.run(op, feed_dict={decoder_input: z})\n",
        "        ax = plt.subplot(g)\n",
        "        img = np.array(x.tolist()).reshape(28, 28)\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_aspect('auto')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def dense(x, n1, n2, name):\n",
        "    \"\"\"\n",
        "    Used to create a dense layer.\n",
        "    :param x: input tensor to the dense layer\n",
        "    :param n1: no. of input neurons\n",
        "    :param n2: no. of output neurons\n",
        "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
        "    :return: tensor with shape [batch_size, n2]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name, reuse=None):\n",
        "        weights = tf.get_variable(\"weights\", shape=[n1, n2],\n",
        "                                  initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n",
        "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
        "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
        "        return out\n",
        "\n",
        "\n",
        "# The autoencoder network\n",
        "def encoder(x, reuse=False):\n",
        "    \"\"\"\n",
        "    Encode part of the autoencoder.\n",
        "    :param x: input to the autoencoder\n",
        "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
        "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "    with tf.name_scope('Encoder'):\n",
        "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
        "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
        "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
        "        return latent_variable\n",
        "\n",
        "\n",
        "def decoder(x, reuse=False):\n",
        "    \"\"\"\n",
        "    Decoder part of the autoencoder.\n",
        "    :param x: input to the decoder\n",
        "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
        "    :return: tensor which should ideally be the input given to the encoder.\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "    with tf.name_scope('Decoder'):\n",
        "        d_dense_1 = tf.nn.relu(dense(x, z_dim, n_l2, 'd_dense_1'))\n",
        "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
        "        output = tf.nn.sigmoid(dense(d_dense_2, n_l1, input_dim, 'd_output'))\n",
        "        return output\n",
        "\n",
        "\n",
        "def discriminator(x, reuse=False):\n",
        "    \"\"\"\n",
        "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
        "    :param x: tensor of shape [batch_size, z_dim]\n",
        "    :param reuse: True -> Reuse the discriminator variables,\n",
        "                  False -> Create or search of variables before creating\n",
        "    :return: tensor of shape [batch_size, 1]\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "    with tf.name_scope('Discriminator'):\n",
        "        dc_den1 = tf.nn.relu(dense(x, z_dim, n_l1, name='dc_den1'))\n",
        "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_den2'))\n",
        "        output = dense(dc_den2, n_l2, 1, name='dc_output')\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(train_model=True):\n",
        "    \"\"\"\n",
        "    Used to train the autoencoder by passing in the necessary inputs.\n",
        "    :param train_model: True -> Train the model, False -> Load the latest trained model and show the image grid.\n",
        "    :return: does not return anything\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(tf.get_variable_scope()):\n",
        "        encoder_output = encoder(x_input)\n",
        "        decoder_output = decoder(encoder_output)\n",
        "\n",
        "    with tf.variable_scope(tf.get_variable_scope()):\n",
        "        d_real = discriminator(real_distribution)\n",
        "        d_fake = discriminator(encoder_output, reuse=True)\n",
        "\n",
        "    with tf.variable_scope(tf.get_variable_scope()):\n",
        "        decoder_image = decoder(decoder_input, reuse=True)\n",
        "\n",
        "    # Autoencoder loss\n",
        "    autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
        "\n",
        "    # Discrimminator Loss\n",
        "    dc_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_real), logits=d_real))\n",
        "    dc_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_fake), logits=d_fake))\n",
        "    dc_loss = dc_loss_fake + dc_loss_real\n",
        "\n",
        "    # Generator loss\n",
        "    generator_loss = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_fake), logits=d_fake))\n",
        "\n",
        "    all_variables = tf.trainable_variables()\n",
        "    dc_var = [var for var in all_variables if 'dc_' in var.name]\n",
        "    en_var = [var for var in all_variables if 'e_' in var.name]\n",
        "\n",
        "    # Optimizers\n",
        "    autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                                   beta1=beta1).minimize(autoencoder_loss)\n",
        "    discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                                     beta1=beta1).minimize(dc_loss, var_list=dc_var)\n",
        "    generator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                                 beta1=beta1).minimize(generator_loss, var_list=en_var)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Reshape immages to display them\n",
        "    input_images = tf.reshape(x_input, [-1, 28, 28, 1])\n",
        "    generated_images = tf.reshape(decoder_output, [-1, 28, 28, 1])\n",
        "\n",
        "    # Tensorboard visualization\n",
        "    tf.summary.scalar(name='Autoencoder Loss', tensor=autoencoder_loss)\n",
        "    tf.summary.scalar(name='Discriminator Loss', tensor=dc_loss)\n",
        "    tf.summary.scalar(name='Generator Loss', tensor=generator_loss)\n",
        "    tf.summary.histogram(name='Encoder Distribution', values=encoder_output)\n",
        "    tf.summary.histogram(name='Real Distribution', values=real_distribution)\n",
        "    tf.summary.image(name='Input Images', tensor=input_images, max_outputs=10)\n",
        "    tf.summary.image(name='Generated Images', tensor=generated_images, max_outputs=10)\n",
        "    summary_op = tf.summary.merge_all()\n",
        "\n",
        "    # Saving the model\n",
        "    saver = tf.train.Saver()\n",
        "    step = 0\n",
        "    with tf.Session() as sess:\n",
        "        if train_model:\n",
        "            tensorboard_path, saved_model_path, log_path = form_results()\n",
        "            sess.run(init)\n",
        "            writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
        "            for i in range(n_epochs):\n",
        "                n_batches = int(X_train.shape[0] / batch_size)\n",
        "                print(\"------------------Epoch {}/{}------------------\".format(i, n_epochs))\n",
        "                for b in range(1, n_batches + 1):\n",
        "                    z_real_dist = np.random.randn(batch_size, z_dim) * 5.\n",
        "                    batch_x, _ = x_input.next_batch(batch_size)\n",
        "                    sess.run(autoencoder_optimizer, feed_dict={x_input: batch_x, x_target: batch_x})\n",
        "                    sess.run(discriminator_optimizer,\n",
        "                             feed_dict={x_input: batch_x, x_target: batch_x, real_distribution: z_real_dist})\n",
        "                    sess.run(generator_optimizer, feed_dict={x_input: batch_x, x_target: batch_x})\n",
        "                    if b % 50 == 0:\n",
        "                        a_loss, d_loss, g_loss, summary = sess.run(\n",
        "                            [autoencoder_loss, dc_loss, generator_loss, summary_op],\n",
        "                            feed_dict={x_input: batch_x, x_target: batch_x,\n",
        "                                       real_distribution: z_real_dist})\n",
        "                        writer.add_summary(summary, global_step=step)\n",
        "                        print(\"Epoch: {}, iteration: {}\".format(i, b))\n",
        "                        print(\"Autoencoder Loss: {}\".format(a_loss))\n",
        "                        print(\"Discriminator Loss: {}\".format(d_loss))\n",
        "                        print(\"Generator Loss: {}\".format(g_loss))\n",
        "                        with open(log_path + '/log.txt', 'a') as log:\n",
        "                            log.write(\"Epoch: {}, iteration: {}\\n\".format(i, b))\n",
        "                            log.write(\"Autoencoder Loss: {}\\n\".format(a_loss))\n",
        "                            log.write(\"Discriminator Loss: {}\\n\".format(d_loss))\n",
        "                            log.write(\"Generator Loss: {}\\n\".format(g_loss))\n",
        "                    step += 1\n",
        "\n",
        "                saver.save(sess, save_path=saved_model_path, global_step=step)\n",
        "        else:\n",
        "            # Get the latest results folder\n",
        "            all_results = os.listdir(results_path)\n",
        "            all_results.sort()\n",
        "            saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/' + all_results[-1] + '/Saved_models/'))\n",
        "            generate_image_grid(sess, op=decoder_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHIv-P6rJfT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f3d1a2e7-afc0-46ae-c858-828d27c0c52a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "X_train, X_test= train_test_split(a,test_size=0.2, random_state=1)\n",
        "\n",
        "x_input, X_val= train_test_split(X_train, test_size=0.2, random_state=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XPAtqHXDHSk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_dim = a.shape[1]\n",
        "n_l1 = 100\n",
        "n_l2 = 100\n",
        "z_dim = 2\n",
        "batch_size = 100\n",
        "n_epochs = 100\n",
        "learning_rate = 0.001\n",
        "beta1 = 0.9\n",
        "results_path = './Results/Adversarial_Autoencoder'\n",
        "\n",
        "# Placeholders for input data and the targets\n",
        "x_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Input')\n",
        "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
        "real_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_dim], name='Real_distribution')\n",
        "decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, z_dim], name='Decoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dcc-6T2JAEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1033
        },
        "outputId": "931575b9-7224-4eab-9bdd-da06399a0f38"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() \n",
        "train(train_model=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-91bd87495026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-b36b4e5152bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_model)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \"\"\"\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b36b4e5152bb>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(x, reuse)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0me_dense_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_dense_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0me_dense_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_dense_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_dense_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlatent_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_dense_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_latent_variable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b36b4e5152bb>\u001b[0m in \u001b[0;36mdense\u001b[0;34m(x, n1, n2, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                   initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n\u001b[1;32m     86\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'matmul'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2039\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m   \"\"\"\n\u001b[0;32m-> 2041\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5770\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5771\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5428\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5429\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5430\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5431\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5364\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5365\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 5366\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   5367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensor(\"e_dense_1/weights:0\", shape=(3721, 100), dtype=float32_ref) must be from the same graph as Tensor(\"Input_1:0\", shape=(100, 3721), dtype=float32)."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "t1zD9IDZ3zQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1414
        },
        "outputId": "3285b525-c324-4b61-b7be-62d42e4f12d1"
      },
      "cell_type": "code",
      "source": [
        "!python3 adversarial_autoencoder.py --train True"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
            "  from ._conv import register_converters as _register_converters\n",
            "WARNING:tensorflow:From adversarial_autoencoder.py:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./Data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./Data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./Data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./Data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "2018-05-16 07:34:47.190525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-05-16 07:34:47.190965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2018-05-16 07:34:47.191005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n",
            "2018-05-16 07:34:47.428190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-05-16 07:34:47.428249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \n",
            "2018-05-16 07:34:47.428277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \n",
            "2018-05-16 07:34:47.428616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10765 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "------------------Epoch 0/100------------------\n",
            "Epoch: 0, iteration: 50\n",
            "Autoencoder Loss: 0.068834587931633\n",
            "Discriminator Loss: 1.1232426166534424\n",
            "Generator Loss: 1.140510082244873\n",
            "Epoch: 0, iteration: 100\n",
            "Autoencoder Loss: 0.06161390617489815\n",
            "Discriminator Loss: 0.6890448331832886\n",
            "Generator Loss: 0.9178003072738647\n",
            "Epoch: 0, iteration: 150\n",
            "Autoencoder Loss: 0.06393840909004211\n",
            "Discriminator Loss: 1.2263400554656982\n",
            "Generator Loss: 1.8126641511917114\n",
            "Epoch: 0, iteration: 200\n",
            "Autoencoder Loss: 0.05874786898493767\n",
            "Discriminator Loss: 0.7781753540039062\n",
            "Generator Loss: 1.1355420351028442\n",
            "Epoch: 0, iteration: 250\n",
            "Autoencoder Loss: 0.059107132256031036\n",
            "Discriminator Loss: 0.9676837921142578\n",
            "Generator Loss: 1.1269699335098267\n",
            "Epoch: 0, iteration: 300\n",
            "Autoencoder Loss: 0.05612283572554588\n",
            "Discriminator Loss: 0.9091423153877258\n",
            "Generator Loss: 1.546984076499939\n",
            "Epoch: 0, iteration: 350\n",
            "Autoencoder Loss: 0.05312090367078781\n",
            "Discriminator Loss: 1.3526571989059448\n",
            "Generator Loss: 0.7582019567489624\n",
            "Epoch: 0, iteration: 400\n",
            "Autoencoder Loss: 0.05169897526502609\n",
            "Discriminator Loss: 1.4284100532531738\n",
            "Generator Loss: 0.8215876817703247\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"adversarial_autoencoder.py\", line 246, in <module>\n",
            "    train(train_model=True)\n",
            "  File \"adversarial_autoencoder.py\", line 214, in train\n",
            "    z_real_dist = np.random.randn(batch_size, z_dim) * 5.\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2lfvZA04JeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1662
        },
        "outputId": "ceaf4777-0e09-40d7-dadb-e12a279c7a5e"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Progressbar\n",
        "# bar = progressbar.ProgressBar(widgets=['[', progressbar.Timer(), ']', progressbar.Bar(), '(', progressbar.ETA(), ')'])\n",
        "\n",
        "# Get the MNIST data\n",
        "mnist = input_data.read_data_sets('./Data', one_hot=True)\n",
        "\n",
        "# Parameters\n",
        "input_dim = 784\n",
        "n_l1 = 1000\n",
        "n_l2 = 1000\n",
        "z_dim = 2\n",
        "batch_size = 100\n",
        "n_epochs = 100\n",
        "learning_rate = 0.001\n",
        "beta1 = 0.9\n",
        "results_path = './Results/Adversarial_Autoencoder'\n",
        "\n",
        "# Placeholders for input data and the targets\n",
        "x_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Input')\n",
        "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
        "real_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_dim], name='Real_distribution')\n",
        "decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, z_dim], name='Decoder_input')\n",
        "\n",
        "\n",
        "def form_results():\n",
        "    \"\"\"\n",
        "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
        "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
        "    \"\"\"\n",
        "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Adversarial_Autoencoder\". \\\n",
        "        format(datetime.datetime.now(), z_dim, learning_rate, batch_size, n_epochs, beta1)\n",
        "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
        "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
        "    log_path = results_path + folder_name + '/log'\n",
        "    if not os.path.exists(results_path + folder_name):\n",
        "        os.mkdir(results_path + folder_name)\n",
        "        os.mkdir(tensorboard_path)\n",
        "        os.mkdir(saved_model_path)\n",
        "        os.mkdir(log_path)\n",
        "    return tensorboard_path, saved_model_path, log_path\n",
        "\n",
        "\n",
        "def generate_image_grid(sess, op):\n",
        "    \"\"\"\n",
        "    Generates a grid of images by passing a set of numbers to the decoder and getting its output.\n",
        "    :param sess: Tensorflow Session required to get the decoder output\n",
        "    :param op: Operation that needs to be called inorder to get the decoder output\n",
        "    :return: None, displays a matplotlib window with all the merged images.\n",
        "    \"\"\"\n",
        "    x_points = np.arange(-10, 10, 1.5).astype(np.float32)\n",
        "    y_points = np.arange(-10, 10, 1.5).astype(np.float32)\n",
        "\n",
        "    nx, ny = len(x_points), len(y_points)\n",
        "    plt.subplot()\n",
        "    gs = gridspec.GridSpec(nx, ny, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i, g in enumerate(gs):\n",
        "        z = np.concatenate(([x_points[int(i / ny)]], [y_points[int(i % nx)]]))\n",
        "        z = np.reshape(z, (1, 2))\n",
        "        x = sess.run(op, feed_dict={decoder_input: z})\n",
        "        ax = plt.subplot(g)\n",
        "        img = np.array(x.tolist()).reshape(28, 28)\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_aspect('auto')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def dense(x, n1, n2, name):\n",
        "    \"\"\"\n",
        "    Used to create a dense layer.\n",
        "    :param x: input tensor to the dense layer\n",
        "    :param n1: no. of input neurons\n",
        "    :param n2: no. of output neurons\n",
        "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
        "    :return: tensor with shape [batch_size, n2]\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name, reuse=None):\n",
        "        weights = tf.get_variable(\"weights\", shape=[n1, n2],\n",
        "                                  initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n",
        "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
        "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
        "        return out\n",
        "\n",
        "\n",
        "# The autoencoder network\n",
        "def encoder(x, reuse=False):\n",
        "    \"\"\"\n",
        "    Encode part of the autoencoder.\n",
        "    :param x: input to the autoencoder\n",
        "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
        "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "    with tf.name_scope('Encoder'):\n",
        "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
        "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
        "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
        "        return latent_variable\n",
        "\n",
        "\n",
        "def decoder(x, reuse=False):\n",
        "    \"\"\"\n",
        "    Decoder part of the autoencoder.\n",
        "    :param x: input to the decoder\n",
        "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
        "    :return: tensor which should ideally be the input given to the encoder.\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "    with tf.name_scope('Decoder'):\n",
        "        d_dense_1 = tf.nn.relu(dense(x, z_dim, n_l2, 'd_dense_1'))\n",
        "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
        "        output = tf.nn.sigmoid(dense(d_dense_2, n_l1, input_dim, 'd_output'))\n",
        "        return output\n",
        "\n",
        "\n",
        "def discriminator(x, reuse=False):\n",
        "    \"\"\"\n",
        "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
        "    :param x: tensor of shape [batch_size, z_dim]\n",
        "    :param reuse: True -> Reuse the discriminator variables,\n",
        "                  False -> Create or search of variables before creating\n",
        "    :return: tensor of shape [batch_size, 1]\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "    with tf.name_scope('Discriminator'):\n",
        "        dc_den1 = tf.nn.relu(dense(x, z_dim, n_l1, name='dc_den1'))\n",
        "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_den2'))\n",
        "        output = dense(dc_den2, n_l2, 1, name='dc_output')\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(train_model=True):\n",
        "    \"\"\"\n",
        "    Used to train the autoencoder by passing in the necessary inputs.\n",
        "    :param train_model: True -> Train the model, False -> Load the latest trained model and show the image grid.\n",
        "    :return: does not return anything\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(tf.get_variable_scope()):\n",
        "        encoder_output = encoder(x_input)\n",
        "        decoder_output = decoder(encoder_output)\n",
        "\n",
        "    with tf.variable_scope(tf.get_variable_scope()):\n",
        "        d_real = discriminator(real_distribution)\n",
        "        d_fake = discriminator(encoder_output, reuse=True)\n",
        "\n",
        "    with tf.variable_scope(tf.get_variable_scope()):\n",
        "        decoder_image = decoder(decoder_input, reuse=True)\n",
        "\n",
        "    # Autoencoder loss\n",
        "    autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
        "\n",
        "    # Discrimminator Loss\n",
        "    dc_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_real), logits=d_real))\n",
        "    dc_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_fake), logits=d_fake))\n",
        "    dc_loss = dc_loss_fake + dc_loss_real\n",
        "\n",
        "    # Generator loss\n",
        "    generator_loss = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_fake), logits=d_fake))\n",
        "\n",
        "    all_variables = tf.trainable_variables()\n",
        "    dc_var = [var for var in all_variables if 'dc_' in var.name]\n",
        "    en_var = [var for var in all_variables if 'e_' in var.name]\n",
        "\n",
        "    # Optimizers\n",
        "    autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                                   beta1=beta1).minimize(autoencoder_loss)\n",
        "    discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                                     beta1=beta1).minimize(dc_loss, var_list=dc_var)\n",
        "    generator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                                 beta1=beta1).minimize(generator_loss, var_list=en_var)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Reshape immages to display them\n",
        "    input_images = tf.reshape(x_input, [-1, 28, 28, 1])\n",
        "    generated_images = tf.reshape(decoder_output, [-1, 28, 28, 1])\n",
        "\n",
        "    # Tensorboard visualization\n",
        "    tf.summary.scalar(name='Autoencoder Loss', tensor=autoencoder_loss)\n",
        "    tf.summary.scalar(name='Discriminator Loss', tensor=dc_loss)\n",
        "    tf.summary.scalar(name='Generator Loss', tensor=generator_loss)\n",
        "    tf.summary.histogram(name='Encoder Distribution', values=encoder_output)\n",
        "    tf.summary.histogram(name='Real Distribution', values=real_distribution)\n",
        "    tf.summary.image(name='Input Images', tensor=input_images, max_outputs=10)\n",
        "    tf.summary.image(name='Generated Images', tensor=generated_images, max_outputs=10)\n",
        "    summary_op = tf.summary.merge_all()\n",
        "\n",
        "    # Saving the model\n",
        "    saver = tf.train.Saver()\n",
        "    step = 0\n",
        "    with tf.Session() as sess:\n",
        "        if train_model:\n",
        "            tensorboard_path, saved_model_path, log_path = form_results()\n",
        "            sess.run(init)\n",
        "            writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
        "            for i in range(n_epochs):\n",
        "                n_batches = int(mnist.train.num_examples / batch_size)\n",
        "                print(\"------------------Epoch {}/{}------------------\".format(i, n_epochs))\n",
        "                for b in range(1, n_batches + 1):\n",
        "                    z_real_dist = np.random.randn(batch_size, z_dim) * 5.\n",
        "                    batch_x, _ = mnist.train.next_batch(batch_size)\n",
        "                    sess.run(autoencoder_optimizer, feed_dict={x_input: batch_x, x_target: batch_x})\n",
        "                    sess.run(discriminator_optimizer,\n",
        "                             feed_dict={x_input: batch_x, x_target: batch_x, real_distribution: z_real_dist})\n",
        "                    sess.run(generator_optimizer, feed_dict={x_input: batch_x, x_target: batch_x})\n",
        "                    if b % 50 == 0:\n",
        "                        a_loss, d_loss, g_loss, summary = sess.run(\n",
        "                            [autoencoder_loss, dc_loss, generator_loss, summary_op],\n",
        "                            feed_dict={x_input: batch_x, x_target: batch_x,\n",
        "                                       real_distribution: z_real_dist})\n",
        "                        writer.add_summary(summary, global_step=step)\n",
        "                        print(\"Epoch: {}, iteration: {}\".format(i, b))\n",
        "                        print(\"Autoencoder Loss: {}\".format(a_loss))\n",
        "                        print(\"Discriminator Loss: {}\".format(d_loss))\n",
        "                        print(\"Generator Loss: {}\".format(g_loss))\n",
        "                        with open(log_path + '/log.txt', 'a') as log:\n",
        "                            log.write(\"Epoch: {}, iteration: {}\\n\".format(i, b))\n",
        "                            log.write(\"Autoencoder Loss: {}\\n\".format(a_loss))\n",
        "                            log.write(\"Discriminator Loss: {}\\n\".format(d_loss))\n",
        "                            log.write(\"Generator Loss: {}\\n\".format(g_loss))\n",
        "                    step += 1\n",
        "\n",
        "                saver.save(sess, save_path=saved_model_path, global_step=step)\n",
        "        else:\n",
        "            # Get the latest results folder\n",
        "            all_results = os.listdir(results_path)\n",
        "            all_results.sort()\n",
        "            saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/' + all_results[-1] + '/Saved_models/'))\n",
        "            generate_image_grid(sess, op=decoder_image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train(train_model=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-33-678e2a34ea5f>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./Data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./Data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ./Data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./Data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-678e2a34ea5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-678e2a34ea5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_model)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-678e2a34ea5f>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(x, reuse)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0me_dense_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_dense_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0me_dense_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_dense_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_dense_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mlatent_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_dense_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e_latent_variable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-678e2a34ea5f>\u001b[0m in \u001b[0;36mdense\u001b[0;34m(x, n1, n2, name)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         weights = tf.get_variable(\"weights\", shape=[n1, n2],\n\u001b[0;32m---> 88\u001b[0;31m                                   initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'matmul'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   def _get_partitioned_variable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 747\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable e_dense_1/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-25-b36b4e5152bb>\", line 85, in dense\n    initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n  File \"<ipython-input-25-b36b4e5152bb>\", line 102, in encoder\n    e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n  File \"<ipython-input-25-b36b4e5152bb>\", line 148, in train\n    encoder_output = encoder(x_input)\n"
          ]
        }
      ]
    }
  ]
}